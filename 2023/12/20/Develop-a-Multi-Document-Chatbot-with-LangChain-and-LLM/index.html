

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jenny Qu">
  <meta name="keywords" content="">
  
    <meta name="description" content="Welcome to this tutorial where we’ll build a powerful chatbot to answer questions from various documents (PDF, DOC, TXT). I’ve used LangChain, OpenAI API, and Large Language Models from Hugging Face t">
<meta property="og:type" content="article">
<meta property="og:title" content="Build Chatbots: Your Own Multi-Document Assistant">
<meta property="og:url" content="http://example.com/2023/12/20/Develop-a-Multi-Document-Chatbot-with-LangChain-and-LLM/index.html">
<meta property="og:site_name" content="Jenny Qu">
<meta property="og:description" content="Welcome to this tutorial where we’ll build a powerful chatbot to answer questions from various documents (PDF, DOC, TXT). I’ve used LangChain, OpenAI API, and Large Language Models from Hugging Face t">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/TP.png">
<meta property="og:image" content="http://example.com/images/Chatbot.jpg">
<meta property="article:published_time" content="2023-12-20T05:41:21.000Z">
<meta property="article:modified_time" content="2024-01-01T10:14:42.337Z">
<meta property="article:author" content="Jenny Qu">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/images/TP.png">
  
  
  
  <title>Build Chatbots: Your Own Multi-Document Assistant - Jenny Qu</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jenny Qu</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About Me</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Build Chatbots: Your Own Multi-Document Assistant"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-12-20 00:41" pubdate>
          December 20, 2023 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          12k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          98 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Build Chatbots: Your Own Multi-Document Assistant</h1>
            
            
              <div class="markdown-body">
                
                <p>Welcome to this tutorial where we’ll build a powerful chatbot to answer questions from various documents (PDF, DOC, TXT). I’ve used <strong>LangChain</strong>, <strong>OpenAI API</strong>, and <strong>Large Language Models from Hugging Face</strong> to create a question&#x2F;answer pipeline, and employed <strong>Streamlit</strong> for crafting a user-friendly web interface. In this blog, I will introduce LangChain, a cutting-edge framework for developing applications using LLMs.</p>
<h2 id="1-Single-Document-vs-Multiple-Documents"><a href="#1-Single-Document-vs-Multiple-Documents" class="headerlink" title="1. Single Document vs Multiple Documents"></a>1. Single Document vs Multiple Documents</h2><p>When dealing with a single document, whether it’s a PDF, Microsoft Word, or a text file, the process remains fairly straightforward. Extracting all the text from the document and feeding it into an LLM prompt like ChatGPT allows us to pose inquiries about the content. This method mirrors the conventional usage of ChatGPT.</p>
<p>However, the scenario becomes more intricate when handling multiple documents. Due to the token limits inherent in LLMs, we confront the challenge of not being able to ingest all the information from these documents in a single request. As a result, our strategy shifts toward sending only the pertinent information to the LLM prompt to circumvent this limitation. But the question arises: How do we isolate and retrieve only the relevant information from our multitude of documents? This is where embeddings and vector stores become pivotal.</p>
<h2 id="2-Embeddings-and-Vector-Stores"><a href="#2-Embeddings-and-Vector-Stores" class="headerlink" title="2. Embeddings and Vector Stores"></a>2. Embeddings and Vector Stores</h2><p>Embeddings and vector stores play a crucial role in distilling relevant information from multiple documents. These tools aid in transforming text into numerical representations that capture semantic relationships and contextual meanings. By converting textual information into high-dimensional vectors, we can effectively organize and index the content. This allows us to efficiently retrieve and present only the most relevant information to the LLM prompt, thereby overcoming the size limitation and ensuring that our queries are focused and contextually aligned.</p>
<h2 id="3-Introduction-to-LangChain"><a href="#3-Introduction-to-LangChain" class="headerlink" title="3. Introduction to LangChain"></a>3. Introduction to LangChain</h2><p>LangChain emerges as a robust framework that equips us with potent tools and methodologies essential for harnessing the capabilities of Language Models (LMs) effectively. It simplifies the implementation of LMs, providing a more user-friendly interface that accelerates the creation of diverse applications leveraging these models. Moreover, LangChain can support various LMs, including those from Hugging Face, OpenAI API, and others.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings  <span class="hljs-comment">#HuggingFaceInstructEmbeddings</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> FAISS<br></code></pre></td></tr></table></figure>
<p><img src="/images/TP.png" srcset="/img/loading.gif" lazyload alt="Documents Processing"></p>
<h3 id="Text-Splitter"><a href="#Text-Splitter" class="headerlink" title="Text Splitter"></a>Text Splitter</h3><p>The <code>text_splitter</code> module within LangChain, exemplified here by the <code>CharacterTextSplitter</code> class, provides a valuable utility for breaking down extensive text into manageable chunks. This functionality becomes particularly useful when dealing with large volumes of text, such as multiple documents, enabling efficient processing and analysis. The parameters defined within <code>CharacterTextSplitter</code>, including the separator, chunk size, and overlap, allow customization to suit specific requirements. By employing this module, developers can segment text intelligently, enhancing the overall workflow efficiency.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_chunks</span>(<span class="hljs-params">text</span>):<br>    text_splitter = CharacterTextSplitter(<br>        separator=<span class="hljs-string">&quot;\n&quot;</span>,<br>        chunk_size=<span class="hljs-number">1000</span>,<br>        chunk_overlap=<span class="hljs-number">200</span>,<br>        length_function=<span class="hljs-built_in">len</span><br>    )<br>    chunks = text_splitter.split_text(text)<br>    <span class="hljs-keyword">return</span> chunks<br></code></pre></td></tr></table></figure>

<h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><p>The utilization of embeddings is integral to LangChain’s functionality. These embeddings capture the semantic nuances and contextual relationships within the text, enabling the generation of high-dimensional vectors that encapsulate the essence of the content. The choice of embeddings can significantly impact the performance of downstream tasks, and LangChain’s flexibility in accommodating various embedding methodologies ensures adaptability to diverse use cases. In our case, we use the OpenAI embeddings transformer, which employs the cosine similarity method to calculate the similarity between documents and a question.</p>
<h3 id="Vector-Stores"><a href="#Vector-Stores" class="headerlink" title="Vector Stores"></a>Vector Stores</h3><p>The vector store, exemplified by the <code>FAISS</code> class, serves as a repository for the high-dimensional vectors generated from the text chunks using the specified embeddings. This component enables efficient storage, indexing, and retrieval of vectors, optimizing the process of accessing relevant information. By organizing these vectors in a manner conducive to rapid search and retrieval, LangChain’s vector stores empower developers to efficiently navigate through vast volumes of textual data, retrieving targeted information while mitigating the challenges posed by token limits in Language Models.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vectorstore</span>(<span class="hljs-params">text_chunks</span>):<br>    embeddings = OpenAIEmbeddings()<br>    <span class="hljs-comment"># embeddings = HuggingFaceInstructEmbeddings(model_name=&quot;hkunlp/instructor-xl&quot;)</span><br>    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)<br>    <span class="hljs-keyword">return</span> vectorstore<br></code></pre></td></tr></table></figure>

<h3 id="LLM-Model-Deployment"><a href="#LLM-Model-Deployment" class="headerlink" title="LLM Model Deployment"></a>LLM Model Deployment</h3><p>Let’s take OpenAI as an example. How do we integrate it into our LangChain?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.chains.question_answering <span class="hljs-keyword">import</span> load_qa_chain<br><br>chain = load_qa_chain(llm=OpenAI())<br>query = <span class="hljs-string">&#x27;Hi, OpenAI.&#x27;</span><br>response = chain.run(input_documents=documents, question=query)<br><span class="hljs-built_in">print</span>(response) <br></code></pre></td></tr></table></figure>

<p>Additionally, we can provide context for the <code>load_qa_chain</code> function, which sends the prompt to OpenAI, resembling something similar to the following:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs applescript">Use <span class="hljs-keyword">the</span> following pieces <span class="hljs-keyword">of</span> context <span class="hljs-keyword">to</span> answer <span class="hljs-keyword">the</span> question <span class="hljs-keyword">at</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">end</span>.<br>If you don&#x27;t know <span class="hljs-keyword">the</span> answer, just <span class="hljs-built_in">say</span> <span class="hljs-keyword">that</span> you don&#x27;t know, don&#x27;t <span class="hljs-keyword">try</span> <span class="hljs-keyword">to</span><br>make up an answer.<br><br>&#123;context&#125;<br>Question: &#123;query&#125;<br>Helpful Answer:<br></code></pre></td></tr></table></figure>

<p>This code snippet demonstrates how we can employ the LangChain framework to load the question-answering chain and utilize an LLM (in this case, OpenAI) to respond to queries based on provided documents.</p>
<h2 id="4-Making-the-Chatbot-Remember-Conversation-History"><a href="#4-Making-the-Chatbot-Remember-Conversation-History" class="headerlink" title="4. Making the Chatbot Remember Conversation History"></a>4. Making the Chatbot Remember Conversation History</h2><p>To elevate the capabilities of our chatbot, we can implement a feature that allows it to retain and recall previous conversation records.</p>
<p>LangChain provides the ConversationBufferMemory class to manage conversation history. This class effectively stores and retrieves dialogue records, passing the history to the model with each request.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ConversationalRetrievalChain<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_conversation_chain</span>(<span class="hljs-params">vectorstore</span>):<br>    llm = ChatOpenAI(temperature=<span class="hljs-number">0.5</span>, max_tokens=<span class="hljs-number">512</span>)  <span class="hljs-comment"># Placeholder for your chosen LLM</span><br>    memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&#x27;chat_history&#x27;</span>, return_messages=<span class="hljs-literal">True</span>)<br>    conversation_chain = ConversationalRetrievalChain.from_llm(<br>        llm=llm,<br>        retriever=vectorstore.as_retriever(),<br>        memory=memory<br>    )<br>    <span class="hljs-keyword">return</span> conversation_chain<br></code></pre></td></tr></table></figure>

<h2 id="5-Streamlit-for-Web-App-Development"><a href="#5-Streamlit-for-Web-App-Development" class="headerlink" title="5. Streamlit for Web App Development"></a>5. Streamlit for Web App Development</h2><p>Streamlit simplifies web app development by enabling users to create interactive applications with ease. Here’s an overview of how you can build a web app using Streamlit:</p>
<ol>
<li><p><strong>Setting up Streamlit</strong>: Install Streamlit using <code>pip install streamlit</code> and initialize your app with <code>streamlit run app.py</code>.</p>
</li>
<li><p><strong>Creating the Interface</strong>:</p>
<ul>
<li>Use <code>st.write()</code> to display text, charts, images, or other content.</li>
<li>Leverage interactive components like <code>st.button()</code>, <code>st.slider()</code>, or <code>st.text_input()</code> for user interaction.</li>
<li>Utilize <code>st.sidebar</code> to create a sidebar for additional controls or information.</li>
</ul>
</li>
<li><p><strong>Handling User Inputs</strong>:</p>
<ul>
<li>Capture user inputs using functions like <code>st.text_input()</code> or <code>st.file_uploader()</code>.</li>
<li>Process and respond to user queries or actions based on the inputs received.</li>
</ul>
</li>
<li><p><strong>Real-time Updates</strong>:</p>
<ul>
<li>Streamlit automatically updates the app in real-time as you modify the code, providing instant previews without manual refreshing.</li>
</ul>
</li>
<li><p><strong>Integration with Data Visualization</strong>:</p>
<ul>
<li>Integrate popular data visualization libraries such as Matplotlib or Plotly to visualize data within your app using <code>st.pyplot()</code> or <code>st.plotly_chart()</code>.</li>
</ul>
</li>
<li><p><strong>Deployment</strong>:</p>
<ul>
<li>Streamlit offers straightforward deployment options for sharing your app, making it accessible to others via a URL.</li>
</ul>
</li>
</ol>
<p>Example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><br><span class="hljs-comment"># App setup</span><br>st.title(<span class="hljs-string">&#x27;My Streamlit Web App&#x27;</span>)<br>user_input = st.text_input(<span class="hljs-string">&#x27;Enter text here:&#x27;</span>)<br>st.write(<span class="hljs-string">&#x27;You entered:&#x27;</span>, user_input)<br></code></pre></td></tr></table></figure>

<p>Streamlit’s simplicity and integration with Python make it an excellent choice for quickly building and deploying web apps, especially for data-driven applications.</p>
<h2 id="6-Entire-Code"><a href="#6-Entire-Code" class="headerlink" title="6. Entire Code"></a>6. Entire Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><span class="hljs-keyword">from</span> dotenv <span class="hljs-keyword">import</span> load_dotenv<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings  <span class="hljs-comment">#HuggingFaceInstructEmbeddings</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> FAISS<br><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ConversationalRetrievalChain<br><span class="hljs-keyword">from</span> htmlTemplates <span class="hljs-keyword">import</span> css, bot_template, user_template<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> HuggingFaceHub<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">docs</span>):<br>    documents = []<br>    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> docs:<br>        text = file.read().decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>        documents.append(text)<br>    <span class="hljs-keyword">return</span> documents<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_chunks</span>(<span class="hljs-params">text</span>):<br>    text_splitter = CharacterTextSplitter(<br>        separator=<span class="hljs-string">&quot;\n&quot;</span>,<br>        chunk_size=<span class="hljs-number">1000</span>,<br>        chunk_overlap=<span class="hljs-number">200</span>,<br>        length_function=<span class="hljs-built_in">len</span><br>    )<br>    chunks = text_splitter.split_text(text)<br>    <span class="hljs-keyword">return</span> chunks<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vectorstore</span>(<span class="hljs-params">text_chunks</span>):<br>    embeddings = OpenAIEmbeddings()<br>    <span class="hljs-comment"># embeddings = HuggingFaceInstructEmbeddings(model_name=&quot;hkunlp/instructor-xl&quot;)</span><br>    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)<br>    <span class="hljs-keyword">return</span> vectorstore<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_conversation_chain</span>(<span class="hljs-params">vectorstore</span>):<br>    llm = ChatOpenAI(temperature=<span class="hljs-number">0.5</span>, max_tokens=<span class="hljs-number">512</span>)<br>    <span class="hljs-comment"># llm = HuggingFaceHub(repo_id=&quot;google/mt5-base&quot;, model_kwargs=&#123;&quot;temperature&quot;:0.5, &quot;max_length&quot;:512&#125;)</span><br>    memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&#x27;chat_history&#x27;</span>, return_messages=<span class="hljs-literal">True</span>)<br>    conversation_chain = ConversationalRetrievalChain.from_llm(<br>        llm=llm,<br>        retriever=vectorstore.as_retriever(),<br>        memory=memory<br>    )<br>    <span class="hljs-keyword">return</span> conversation_chain<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_userinput</span>(<span class="hljs-params">user_question</span>):<br>    response = st.session_state.conversation(&#123;<span class="hljs-string">&#x27;question&#x27;</span>: user_question&#125;)<br>    st.session_state.chat_history = response[<span class="hljs-string">&#x27;chat_history&#x27;</span>]<br><br>    <span class="hljs-keyword">for</span> i, message <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(st.session_state.chat_history):<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>            st.write(user_template.replace(<br>                <span class="hljs-string">&quot;&#123;&#123;MSG&#125;&#125;&quot;</span>, message.content), unsafe_allow_html=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">else</span>:<br>            st.write(bot_template.replace(<br>                <span class="hljs-string">&quot;&#123;&#123;MSG&#125;&#125;&quot;</span>, message.content), unsafe_allow_html=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    load_dotenv()<br>    st.set_page_config(page_title=<span class="hljs-string">&quot;Chat with docs&quot;</span>, page_icon=<span class="hljs-string">&#x27;:books:&#x27;</span>)<br>    st.write(css, unsafe_allow_html=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;conversation&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        st.session_state.conversation = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;chat_history&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        st.session_state.chat_history = <span class="hljs-literal">None</span><br><br>    st.header(<span class="hljs-string">&quot;Chat with docs :books:&quot;</span>)<br>    user_question = st.text_input(<span class="hljs-string">&quot;Ask a question about your documents:&quot;</span>)<br>    <span class="hljs-keyword">if</span> user_question:<br>        handle_userinput(user_question)<br>    <br>    <span class="hljs-keyword">with</span> st.sidebar:<br>        st.subheader(<span class="hljs-string">&quot;your docs&quot;</span>)<br>        docs=st.file_uploader(<span class="hljs-string">&quot;Upload here&quot;</span>, accept_multiple_files=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> st.button(<span class="hljs-string">&quot;Process&quot;</span>):<br>            <span class="hljs-keyword">with</span> st.spinner(<span class="hljs-string">&quot;Processing&quot;</span>):<br>                <span class="hljs-comment">#get the text</span><br>                raw_text=get_text(docs)<br>                <span class="hljs-comment">#get the chunks</span><br>                text_chunks=get_text_chunks(raw_text)<br>                <span class="hljs-comment"># st.write(text_chunks)</span><br>                <span class="hljs-comment">#create vector store</span><br>                vectorstore=get_vectorstore(text_chunks)<br>                <span class="hljs-comment"># create conversation chain</span><br>                st.session_state.conversation = get_conversation_chain(vectorstore)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure>

<p>This code specifically handles text files (.txt). For PDF or DOC files, you’d need to utilize specific libraries to extract text content from them.<br>For PDF files, libraries like PyPDF2, pdfplumber, or PyMuPDF can be used to extract text content.<br>For DOC files, libraries such as python-docx, pywin32, or textract can assist in obtaining text content.<br>For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">documents = []<br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(<span class="hljs-string">&#x27;docs&#x27;</span>):<br>    <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.pdf&#x27;</span>):<br>        pdf_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = PyPDFLoader(pdf_path)<br>        documents.extend(loader.load())<br>    <span class="hljs-keyword">elif</span> file.endswith(<span class="hljs-string">&#x27;.docx&#x27;</span>) <span class="hljs-keyword">or</span> file.endswith(<span class="hljs-string">&#x27;.doc&#x27;</span>):<br>        doc_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = Docx2txtLoader(doc_path)<br>        documents.extend(loader.load())<br>    <span class="hljs-keyword">elif</span> file.endswith(<span class="hljs-string">&#x27;.txt&#x27;</span>):<br>        text_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = TextLoader(text_path)<br>        documents.extend(loader.load())<br></code></pre></td></tr></table></figure>

<p>Additionally, you need to input your <code>OPENAI_API_KEY</code> and <code>HUGGINGFACEHUB_API_TOKEN</code> in the <code>.env</code> file. You’ll also require an <code>htmlTemplates.py</code>. For specific project details, please refer to my Chatbot repository on GitHub (publicly available).</p>
<h2 id="7-Running-Examples"><a href="#7-Running-Examples" class="headerlink" title="7. Running Examples"></a>7. Running Examples</h2><p>Open your terminal and input <code>streamlit run</code> followed by the file path. For instance, on my computer:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">streamlit run /Users/jenny/Documents/chatbot/app.py<br></code></pre></td></tr></table></figure>

<p>This command starts the application by executing the file named <code>app.py</code> located at the specified file path.</p>
<p>After uploading your files, click on the ‘Process’ button, then you can ask your questions!</p>
<p>This is a screenshot of my running example:</p>
<p><img src="/images/Chatbot.jpg" srcset="/img/loading.gif" lazyload alt="Running Example"></p>
<p>Happy coding!</p>
<!-- 
cd documents/chatbot/
jenny@JennydeMacBook-Air chatbot % echo 'export PATH=/Users/jenny/Library/Python/3.9/bin:$PATH' >>~/.bashrc
jenny@JennydeMacBook-Air chatbot % source ~/.bashrc 
-->
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Natural-language-processing/" class="category-chain-item">Natural language processing</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI/" class="print-no-link">#AI</a>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/NLP/" class="print-no-link">#NLP</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Build Chatbots: Your Own Multi-Document Assistant</div>
      <div>http://example.com/2023/12/20/Develop-a-Multi-Document-Chatbot-with-LangChain-and-LLM/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Jenny Qu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>December 20, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/09/01/About-Me/" title="About me">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">About me</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/10/28/NLP-Sentiment-Analysis-with-LSTM/" title="NLP: Sentiment Analysis with LSTM">
                        <span class="hidden-mobile">NLP: Sentiment Analysis with LSTM</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a target="_blank" rel="noopener" href="https://info.flagcounter.com/TNBh"> <img src="https://s01.flagcounter.com/count2/TNBh/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" srcset="/img/loading.gif" lazyload alt="Flag Counter" border="0"> </a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
