<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>About me</title>
    <link href="/2024/09/01/About-Me/"/>
    <url>/2024/09/01/About-Me/</url>
    
    <content type="html"><![CDATA[<p>Hi there, I’m Jenny Qu, a graduate student in <strong>Software Engineering</strong> at the School of Computer Science at Carnegie Mellon University. During my undergraduate studies, I majored in <strong>Finance</strong>. With an interdisciplinary background, I am eager to apply my technical skills in software engineering, data science, and financial knowledge to create products, making advanced technologies more accessible and beneficial to the public.</p><p>In my free time, I enjoy trying new foods, baking, and swimming.</p><p>Thanks for visiting my website! This blog is where I share my learning experiences and insights from my journey in tech. For further interaction, feel free to reach out to me via email or LinkedIn. Let’s make progress together!</p><h3 id="Professional-Experience"><a href="#Professional-Experience" class="headerlink" title="Professional Experience"></a>Professional Experience</h3><h4 id="Mastercard-Data-Engineer-Intern"><a href="#Mastercard-Data-Engineer-Intern" class="headerlink" title="Mastercard - Data Engineer Intern"></a><strong><a href="https://www.mastercard.com/global/en.html">Mastercard</a> - Data Engineer Intern</strong></h4><ul><li>Hive, Python, SQL, Excel PivotTable, ETL, Git</li><li><strong>Skills</strong>: Problem-solving, Data analysis, Data visualization, Effective communication</li></ul><h4 id="Meituan-Business-Data-Analyst-Intern"><a href="#Meituan-Business-Data-Analyst-Intern" class="headerlink" title="Meituan - Business Data Analyst Intern"></a><strong><a href="https://www.meituan.com/en-US/about-us">Meituan</a> - Business Data Analyst Intern</strong></h4><ul><li>Spark, Python, SQL, Tableau, Scikit-Learn, Git</li><li><strong>Skills</strong>: Machine Learning, Regression prediction, A&#x2F;B testing, Teamwork</li></ul><h4 id="China-Life-Investment-Investment-Analyst-Intern"><a href="#China-Life-Investment-Investment-Analyst-Intern" class="headerlink" title="China Life Investment - Investment Analyst Intern"></a><strong><a href="https://www.chinalifeinvest.com/">China Life Investment</a> - Investment Analyst Intern</strong></h4><ul><li>VBA, Excel, Bloomberg, Capital IQ</li><li><strong>Skills</strong>: Due diligence, Equity valuation, Risk assessment, Report writing, Presentation</li></ul><h3 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h3><h4 id="EasyBook-Full-Stack-Hotel-Booking-App"><a href="#EasyBook-Full-Stack-Hotel-Booking-App" class="headerlink" title="EasyBook: Full-Stack Hotel Booking App"></a><strong>EasyBook: Full-Stack Hotel Booking App</strong></h4><ul><li>Java, Spring Boot, Restful APT, MySQL, React, Bootstrap, Axios, Postman, Spring Security, JWT</li></ul><h4 id="Data-Warehouse-Project"><a href="#Data-Warehouse-Project" class="headerlink" title="Data Warehouse Project"></a><strong>Data Warehouse Project</strong></h4><ul><li>Hadoop, Sqoop, Flume, Azkaban, HDFS, MySQL</li></ul><h4 id="Multi-Document-Reader-and-Chatbot"><a href="#Multi-Document-Reader-and-Chatbot" class="headerlink" title="Multi-Document Reader and Chatbot"></a><strong><a href="https://jennymyworld.github.io/2023/12/20/Develop-a-Multi-Document-Chatbot-with-LangChain-and-LLM/">Multi-Document Reader and Chatbot</a></strong></h4><ul><li>LangChain, OpenAI API, Large Language Models, Hugging Face, Streamlit<!-- <a href="https://info.flagcounter.com/TNBh"><img src="https://s01.flagcounter.com/count2/TNBh/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a> --></li></ul>]]></content>
    
    
    <categories>
      
      <category>Website</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Website</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Build Chatbots: Your Own Multi-Document Assistant</title>
    <link href="/2023/12/20/Develop-a-Multi-Document-Chatbot-with-LangChain-and-LLM/"/>
    <url>/2023/12/20/Develop-a-Multi-Document-Chatbot-with-LangChain-and-LLM/</url>
    
    <content type="html"><![CDATA[<p>Welcome to this tutorial where we’ll build a powerful chatbot to answer questions from various documents (PDF, DOC, TXT). I’ve used <strong>LangChain</strong>, <strong>OpenAI API</strong>, and <strong>Large Language Models from Hugging Face</strong> to create a question&#x2F;answer pipeline, and employed <strong>Streamlit</strong> for crafting a user-friendly web interface. In this blog, I will introduce LangChain, a cutting-edge framework for developing applications using LLMs.</p><h2 id="1-Single-Document-vs-Multiple-Documents"><a href="#1-Single-Document-vs-Multiple-Documents" class="headerlink" title="1. Single Document vs Multiple Documents"></a>1. Single Document vs Multiple Documents</h2><p>When dealing with a single document, whether it’s a PDF, Microsoft Word, or a text file, the process remains fairly straightforward. Extracting all the text from the document and feeding it into an LLM prompt like ChatGPT allows us to pose inquiries about the content. This method mirrors the conventional usage of ChatGPT.</p><p>However, the scenario becomes more intricate when handling multiple documents. Due to the token limits inherent in LLMs, we confront the challenge of not being able to ingest all the information from these documents in a single request. As a result, our strategy shifts toward sending only the pertinent information to the LLM prompt to circumvent this limitation. But the question arises: How do we isolate and retrieve only the relevant information from our multitude of documents? This is where embeddings and vector stores become pivotal.</p><h2 id="2-Embeddings-and-Vector-Stores"><a href="#2-Embeddings-and-Vector-Stores" class="headerlink" title="2. Embeddings and Vector Stores"></a>2. Embeddings and Vector Stores</h2><p>Embeddings and vector stores play a crucial role in distilling relevant information from multiple documents. These tools aid in transforming text into numerical representations that capture semantic relationships and contextual meanings. By converting textual information into high-dimensional vectors, we can effectively organize and index the content. This allows us to efficiently retrieve and present only the most relevant information to the LLM prompt, thereby overcoming the size limitation and ensuring that our queries are focused and contextually aligned.</p><h2 id="3-Introduction-to-LangChain"><a href="#3-Introduction-to-LangChain" class="headerlink" title="3. Introduction to LangChain"></a>3. Introduction to LangChain</h2><p>LangChain emerges as a robust framework that equips us with potent tools and methodologies essential for harnessing the capabilities of Language Models (LMs) effectively. It simplifies the implementation of LMs, providing a more user-friendly interface that accelerates the creation of diverse applications leveraging these models. Moreover, LangChain can support various LMs, including those from Hugging Face, OpenAI API, and others.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings  <span class="hljs-comment">#HuggingFaceInstructEmbeddings</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> FAISS<br></code></pre></td></tr></table></figure><p><img src="/images/TP.png" alt="Documents Processing"></p><h3 id="Text-Splitter"><a href="#Text-Splitter" class="headerlink" title="Text Splitter"></a>Text Splitter</h3><p>The <code>text_splitter</code> module within LangChain, exemplified here by the <code>CharacterTextSplitter</code> class, provides a valuable utility for breaking down extensive text into manageable chunks. This functionality becomes particularly useful when dealing with large volumes of text, such as multiple documents, enabling efficient processing and analysis. The parameters defined within <code>CharacterTextSplitter</code>, including the separator, chunk size, and overlap, allow customization to suit specific requirements. By employing this module, developers can segment text intelligently, enhancing the overall workflow efficiency.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_chunks</span>(<span class="hljs-params">text</span>):<br>    text_splitter = CharacterTextSplitter(<br>        separator=<span class="hljs-string">&quot;\n&quot;</span>,<br>        chunk_size=<span class="hljs-number">1000</span>,<br>        chunk_overlap=<span class="hljs-number">200</span>,<br>        length_function=<span class="hljs-built_in">len</span><br>    )<br>    chunks = text_splitter.split_text(text)<br>    <span class="hljs-keyword">return</span> chunks<br></code></pre></td></tr></table></figure><h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><p>The utilization of embeddings is integral to LangChain’s functionality. These embeddings capture the semantic nuances and contextual relationships within the text, enabling the generation of high-dimensional vectors that encapsulate the essence of the content. The choice of embeddings can significantly impact the performance of downstream tasks, and LangChain’s flexibility in accommodating various embedding methodologies ensures adaptability to diverse use cases. In our case, we use the OpenAI embeddings transformer, which employs the cosine similarity method to calculate the similarity between documents and a question.</p><h3 id="Vector-Stores"><a href="#Vector-Stores" class="headerlink" title="Vector Stores"></a>Vector Stores</h3><p>The vector store, exemplified by the <code>FAISS</code> class, serves as a repository for the high-dimensional vectors generated from the text chunks using the specified embeddings. This component enables efficient storage, indexing, and retrieval of vectors, optimizing the process of accessing relevant information. By organizing these vectors in a manner conducive to rapid search and retrieval, LangChain’s vector stores empower developers to efficiently navigate through vast volumes of textual data, retrieving targeted information while mitigating the challenges posed by token limits in Language Models.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vectorstore</span>(<span class="hljs-params">text_chunks</span>):<br>    embeddings = OpenAIEmbeddings()<br>    <span class="hljs-comment"># embeddings = HuggingFaceInstructEmbeddings(model_name=&quot;hkunlp/instructor-xl&quot;)</span><br>    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)<br>    <span class="hljs-keyword">return</span> vectorstore<br></code></pre></td></tr></table></figure><h3 id="LLM-Model-Deployment"><a href="#LLM-Model-Deployment" class="headerlink" title="LLM Model Deployment"></a>LLM Model Deployment</h3><p>Let’s take OpenAI as an example. How do we integrate it into our LangChain?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> OpenAI<br><span class="hljs-keyword">from</span> langchain.chains.question_answering <span class="hljs-keyword">import</span> load_qa_chain<br><br>chain = load_qa_chain(llm=OpenAI())<br>query = <span class="hljs-string">&#x27;Hi, OpenAI.&#x27;</span><br>response = chain.run(input_documents=documents, question=query)<br><span class="hljs-built_in">print</span>(response) <br></code></pre></td></tr></table></figure><p>Additionally, we can provide context for the <code>load_qa_chain</code> function, which sends the prompt to OpenAI, resembling something similar to the following:</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs applescript">Use <span class="hljs-keyword">the</span> following pieces <span class="hljs-keyword">of</span> context <span class="hljs-keyword">to</span> answer <span class="hljs-keyword">the</span> question <span class="hljs-keyword">at</span> <span class="hljs-keyword">the</span> <span class="hljs-keyword">end</span>.<br>If you don&#x27;t know <span class="hljs-keyword">the</span> answer, just <span class="hljs-built_in">say</span> <span class="hljs-keyword">that</span> you don&#x27;t know, don&#x27;t <span class="hljs-keyword">try</span> <span class="hljs-keyword">to</span><br>make up an answer.<br><br>&#123;context&#125;<br>Question: &#123;query&#125;<br>Helpful Answer:<br></code></pre></td></tr></table></figure><p>This code snippet demonstrates how we can employ the LangChain framework to load the question-answering chain and utilize an LLM (in this case, OpenAI) to respond to queries based on provided documents.</p><h2 id="4-Making-the-Chatbot-Remember-Conversation-History"><a href="#4-Making-the-Chatbot-Remember-Conversation-History" class="headerlink" title="4. Making the Chatbot Remember Conversation History"></a>4. Making the Chatbot Remember Conversation History</h2><p>To elevate the capabilities of our chatbot, we can implement a feature that allows it to retain and recall previous conversation records.</p><p>LangChain provides the ConversationBufferMemory class to manage conversation history. This class effectively stores and retrieves dialogue records, passing the history to the model with each request.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ConversationalRetrievalChain<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_conversation_chain</span>(<span class="hljs-params">vectorstore</span>):<br>    llm = ChatOpenAI(temperature=<span class="hljs-number">0.5</span>, max_tokens=<span class="hljs-number">512</span>)  <span class="hljs-comment"># Placeholder for your chosen LLM</span><br>    memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&#x27;chat_history&#x27;</span>, return_messages=<span class="hljs-literal">True</span>)<br>    conversation_chain = ConversationalRetrievalChain.from_llm(<br>        llm=llm,<br>        retriever=vectorstore.as_retriever(),<br>        memory=memory<br>    )<br>    <span class="hljs-keyword">return</span> conversation_chain<br></code></pre></td></tr></table></figure><h2 id="5-Streamlit-for-Web-App-Development"><a href="#5-Streamlit-for-Web-App-Development" class="headerlink" title="5. Streamlit for Web App Development"></a>5. Streamlit for Web App Development</h2><p>Streamlit simplifies web app development by enabling users to create interactive applications with ease. Here’s an overview of how you can build a web app using Streamlit:</p><ol><li><p><strong>Setting up Streamlit</strong>: Install Streamlit using <code>pip install streamlit</code> and initialize your app with <code>streamlit run app.py</code>.</p></li><li><p><strong>Creating the Interface</strong>:</p><ul><li>Use <code>st.write()</code> to display text, charts, images, or other content.</li><li>Leverage interactive components like <code>st.button()</code>, <code>st.slider()</code>, or <code>st.text_input()</code> for user interaction.</li><li>Utilize <code>st.sidebar</code> to create a sidebar for additional controls or information.</li></ul></li><li><p><strong>Handling User Inputs</strong>:</p><ul><li>Capture user inputs using functions like <code>st.text_input()</code> or <code>st.file_uploader()</code>.</li><li>Process and respond to user queries or actions based on the inputs received.</li></ul></li><li><p><strong>Real-time Updates</strong>:</p><ul><li>Streamlit automatically updates the app in real-time as you modify the code, providing instant previews without manual refreshing.</li></ul></li><li><p><strong>Integration with Data Visualization</strong>:</p><ul><li>Integrate popular data visualization libraries such as Matplotlib or Plotly to visualize data within your app using <code>st.pyplot()</code> or <code>st.plotly_chart()</code>.</li></ul></li><li><p><strong>Deployment</strong>:</p><ul><li>Streamlit offers straightforward deployment options for sharing your app, making it accessible to others via a URL.</li></ul></li></ol><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><br><span class="hljs-comment"># App setup</span><br>st.title(<span class="hljs-string">&#x27;My Streamlit Web App&#x27;</span>)<br>user_input = st.text_input(<span class="hljs-string">&#x27;Enter text here:&#x27;</span>)<br>st.write(<span class="hljs-string">&#x27;You entered:&#x27;</span>, user_input)<br></code></pre></td></tr></table></figure><p>Streamlit’s simplicity and integration with Python make it an excellent choice for quickly building and deploying web apps, especially for data-driven applications.</p><h2 id="6-Entire-Code"><a href="#6-Entire-Code" class="headerlink" title="6. Entire Code"></a>6. Entire Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> streamlit <span class="hljs-keyword">as</span> st<br><span class="hljs-keyword">from</span> dotenv <span class="hljs-keyword">import</span> load_dotenv<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> CharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.embeddings <span class="hljs-keyword">import</span> OpenAIEmbeddings  <span class="hljs-comment">#HuggingFaceInstructEmbeddings</span><br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> FAISS<br><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI<br><span class="hljs-keyword">from</span> langchain.memory <span class="hljs-keyword">import</span> ConversationBufferMemory<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> ConversationalRetrievalChain<br><span class="hljs-keyword">from</span> htmlTemplates <span class="hljs-keyword">import</span> css, bot_template, user_template<br><span class="hljs-keyword">from</span> langchain.llms <span class="hljs-keyword">import</span> HuggingFaceHub<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">docs</span>):<br>    documents = []<br>    <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> docs:<br>        text = file.read().decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>        documents.append(text)<br>    <span class="hljs-keyword">return</span> documents<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text_chunks</span>(<span class="hljs-params">text</span>):<br>    text_splitter = CharacterTextSplitter(<br>        separator=<span class="hljs-string">&quot;\n&quot;</span>,<br>        chunk_size=<span class="hljs-number">1000</span>,<br>        chunk_overlap=<span class="hljs-number">200</span>,<br>        length_function=<span class="hljs-built_in">len</span><br>    )<br>    chunks = text_splitter.split_text(text)<br>    <span class="hljs-keyword">return</span> chunks<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_vectorstore</span>(<span class="hljs-params">text_chunks</span>):<br>    embeddings = OpenAIEmbeddings()<br>    <span class="hljs-comment"># embeddings = HuggingFaceInstructEmbeddings(model_name=&quot;hkunlp/instructor-xl&quot;)</span><br>    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)<br>    <span class="hljs-keyword">return</span> vectorstore<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_conversation_chain</span>(<span class="hljs-params">vectorstore</span>):<br>    llm = ChatOpenAI(temperature=<span class="hljs-number">0.5</span>, max_tokens=<span class="hljs-number">512</span>)<br>    <span class="hljs-comment"># llm = HuggingFaceHub(repo_id=&quot;google/mt5-base&quot;, model_kwargs=&#123;&quot;temperature&quot;:0.5, &quot;max_length&quot;:512&#125;)</span><br>    memory = ConversationBufferMemory(memory_key=<span class="hljs-string">&#x27;chat_history&#x27;</span>, return_messages=<span class="hljs-literal">True</span>)<br>    conversation_chain = ConversationalRetrievalChain.from_llm(<br>        llm=llm,<br>        retriever=vectorstore.as_retriever(),<br>        memory=memory<br>    )<br>    <span class="hljs-keyword">return</span> conversation_chain<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_userinput</span>(<span class="hljs-params">user_question</span>):<br>    response = st.session_state.conversation(&#123;<span class="hljs-string">&#x27;question&#x27;</span>: user_question&#125;)<br>    st.session_state.chat_history = response[<span class="hljs-string">&#x27;chat_history&#x27;</span>]<br><br>    <span class="hljs-keyword">for</span> i, message <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(st.session_state.chat_history):<br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>            st.write(user_template.replace(<br>                <span class="hljs-string">&quot;&#123;&#123;MSG&#125;&#125;&quot;</span>, message.content), unsafe_allow_html=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">else</span>:<br>            st.write(bot_template.replace(<br>                <span class="hljs-string">&quot;&#123;&#123;MSG&#125;&#125;&quot;</span>, message.content), unsafe_allow_html=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    load_dotenv()<br>    st.set_page_config(page_title=<span class="hljs-string">&quot;Chat with docs&quot;</span>, page_icon=<span class="hljs-string">&#x27;:books:&#x27;</span>)<br>    st.write(css, unsafe_allow_html=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;conversation&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        st.session_state.conversation = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;chat_history&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:<br>        st.session_state.chat_history = <span class="hljs-literal">None</span><br><br>    st.header(<span class="hljs-string">&quot;Chat with docs :books:&quot;</span>)<br>    user_question = st.text_input(<span class="hljs-string">&quot;Ask a question about your documents:&quot;</span>)<br>    <span class="hljs-keyword">if</span> user_question:<br>        handle_userinput(user_question)<br>    <br>    <span class="hljs-keyword">with</span> st.sidebar:<br>        st.subheader(<span class="hljs-string">&quot;your docs&quot;</span>)<br>        docs=st.file_uploader(<span class="hljs-string">&quot;Upload here&quot;</span>, accept_multiple_files=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> st.button(<span class="hljs-string">&quot;Process&quot;</span>):<br>            <span class="hljs-keyword">with</span> st.spinner(<span class="hljs-string">&quot;Processing&quot;</span>):<br>                <span class="hljs-comment">#get the text</span><br>                raw_text=get_text(docs)<br>                <span class="hljs-comment">#get the chunks</span><br>                text_chunks=get_text_chunks(raw_text)<br>                <span class="hljs-comment"># st.write(text_chunks)</span><br>                <span class="hljs-comment">#create vector store</span><br>                vectorstore=get_vectorstore(text_chunks)<br>                <span class="hljs-comment"># create conversation chain</span><br>                st.session_state.conversation = get_conversation_chain(vectorstore)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure><p>This code specifically handles text files (.txt). For PDF or DOC files, you’d need to utilize specific libraries to extract text content from them.<br>For PDF files, libraries like PyPDF2, pdfplumber, or PyMuPDF can be used to extract text content.<br>For DOC files, libraries such as python-docx, pywin32, or textract can assist in obtaining text content.<br>For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">documents = []<br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(<span class="hljs-string">&#x27;docs&#x27;</span>):<br>    <span class="hljs-keyword">if</span> file.endswith(<span class="hljs-string">&#x27;.pdf&#x27;</span>):<br>        pdf_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = PyPDFLoader(pdf_path)<br>        documents.extend(loader.load())<br>    <span class="hljs-keyword">elif</span> file.endswith(<span class="hljs-string">&#x27;.docx&#x27;</span>) <span class="hljs-keyword">or</span> file.endswith(<span class="hljs-string">&#x27;.doc&#x27;</span>):<br>        doc_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = Docx2txtLoader(doc_path)<br>        documents.extend(loader.load())<br>    <span class="hljs-keyword">elif</span> file.endswith(<span class="hljs-string">&#x27;.txt&#x27;</span>):<br>        text_path = <span class="hljs-string">&#x27;./docs/&#x27;</span> + file<br>        loader = TextLoader(text_path)<br>        documents.extend(loader.load())<br></code></pre></td></tr></table></figure><p>Additionally, you need to input your <code>OPENAI_API_KEY</code> and <code>HUGGINGFACEHUB_API_TOKEN</code> in the <code>.env</code> file. You’ll also require an <code>htmlTemplates.py</code>. For specific project details, please refer to my Chatbot repository on GitHub (publicly available).</p><h2 id="7-Running-Examples"><a href="#7-Running-Examples" class="headerlink" title="7. Running Examples"></a>7. Running Examples</h2><p>Open your terminal and input <code>streamlit run</code> followed by the file path. For instance, on my computer:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">streamlit run /Users/jenny/Documents/chatbot/app.py<br></code></pre></td></tr></table></figure><p>This command starts the application by executing the file named <code>app.py</code> located at the specified file path.</p><p>After uploading your files, click on the ‘Process’ button, then you can ask your questions!</p><p>This is a screenshot of my running example:</p><p><img src="/images/Chatbot.jpg" alt="Running Example"></p><p>Happy coding!</p><!-- cd documents/chatbot/jenny@JennydeMacBook-Air chatbot % echo 'export PATH=/Users/jenny/Library/Python/3.9/bin:$PATH' >>~/.bashrcjenny@JennydeMacBook-Air chatbot % source ~/.bashrc -->]]></content>
    
    
    <categories>
      
      <category>Natural language processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP: Sentiment Analysis with LSTM</title>
    <link href="/2023/10/28/NLP-Sentiment-Analysis-with-LSTM/"/>
    <url>/2023/10/28/NLP-Sentiment-Analysis-with-LSTM/</url>
    
    <content type="html"><![CDATA[<h3 id="Introduction-to-Sentiment-Analysis"><a href="#Introduction-to-Sentiment-Analysis" class="headerlink" title="Introduction to Sentiment Analysis"></a>Introduction to Sentiment Analysis</h3><p>Sentiment Analysis, a common application in Natural Language Processing (NLP), is an interesting task aimed at extracting emotional content from text and categorizing it. It involves analyzing, processing, summarizing, and inferring subjective text with emotional tones.</p><p>This article will focus on sentiment polarity analysis within sentiment analysis. Sentiment polarity analysis refers to categorizing text into positive, negative, or neutral sentiments. In most applications, sentiments are classified into two categories. For example, words like “like” and “dislike” represent different emotional orientations.</p><p>This article will delve into using the LSTM model, a deep learning model, for sentiment analysis on Chinese text.</p><h3 id="Text-Introduction-and-Corpus-Analysis"><a href="#Text-Introduction-and-Corpus-Analysis" class="headerlink" title="Text Introduction and Corpus Analysis"></a>Text Introduction and Corpus Analysis</h3><p>We’ll use comments about a product from an e-commerce website as our corpus (corpus.csv), which can be downloaded from <a href="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">this link</a>. The dataset consists of 4310 comment entries, categorized as “positive” and “negative”. Here are a few entries from the dataset:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs markdown">evaluation,label<br>用了一段时间，感觉还不错，可以,正面<br>电视非常好，已经是家里的第二台了。第一天下单，第二天就到本地了，可是物流的人说车坏了，一直催，客服也帮着催，到第三天下午5点才送过来。父母年纪大了，买个大电视画面清晰，趁着耳朵还好使，享受几年。,正面<br>电视比想象中的大好多，画面也很清晰，系统很智能，更多功能还在摸索中,正面<br>不错,正面<br>用了这么多天了，感觉还不错。夏普的牌子还是比较可靠。希望以后比较耐用，现在是考量质量的时候。,正面<br>物流速度很快，非常棒，今天就看了电视，非常清晰，非常流畅，一次非常完美的购物体验,正面<br>非常好，客服还特意打电话做回访,正面<br>物流小哥不错，辛苦了，东西还没用,正面<br>送货速度快，质量有保障，活动价格挺好的。希望用的久，不出问题。,正面<br></code></pre></td></tr></table></figure><p>Following this, we perform a simple analysis on the corpus:</p><ul><li>Distribution of sentiments in the dataset.</li><li>Distribution of comment sentence lengths in the dataset.</li></ul><p>We use the following Python script to analyze the sentiment distribution and the length distribution of comment sentences.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> font_manager<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><br><span class="hljs-comment"># Set the font for matplotlib plots</span><br>my_font = font_manager.FontProperties(fname=<span class="hljs-string">&quot;/Library/Fonts/Songti.ttc&quot;</span>)<br><br><span class="hljs-comment"># Calculate sentence lengths and their frequency</span><br>df = pd.read_csv(<span class="hljs-string">&#x27;./corpus.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(df.groupby(<span class="hljs-string">&#x27;label&#x27;</span>)[<span class="hljs-string">&#x27;label&#x27;</span>].count())<br><br>df[<span class="hljs-string">&#x27;length&#x27;</span>] = df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br>len_df = df.groupby(<span class="hljs-string">&#x27;length&#x27;</span>).count()<br>sent_length = len_df.index.tolist()<br>sent_freq = len_df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].tolist()<br><br><span class="hljs-comment"># Plot the histogram of sentence lengths and their frequency</span><br>plt.bar(sent_length, sent_freq)<br>plt.title(<span class="hljs-string">&quot;Histogram of Sentence Lengths and Frequency&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;Sentence Length&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;Frequency of Sentence Length&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./Histogram_of_Sentence_Lengths.png&quot;</span>)<br>plt.close()<br><br><span class="hljs-comment"># Plot the Cumulative Distribution Function (CDF) of sentence lengths</span><br>sent_percentage_list = [(count/<span class="hljs-built_in">sum</span>(sent_freq)) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_freq)]<br>plt.plot(sent_length, sent_percentage_list)<br><br><span class="hljs-comment"># Find the sentence length at quantile percentile</span><br>quantile = <span class="hljs-number">0.91</span><br><span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_length, sent_percentage_list):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">2</span>) == quantile:<br>        index = length<br>        <span class="hljs-keyword">break</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nSentence length at %s quantile: %d.&quot;</span> % (quantile, index))<br><br><span class="hljs-comment"># Plot the CDF of sentence lengths</span><br>plt.plot(sent_length, sent_percentage_list)<br>plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>plt.title(<span class="hljs-string">&quot;Cumulative Distribution Function of Sentence Lengths&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;Sentence Length&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;Cumulative Frequency of Sentence Length&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./CDF_of_Sentence_Lengths.png&quot;</span>)<br>plt.close()<br><br></code></pre></td></tr></table></figure><p>The output result would be:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown">label<br>Positive    1908<br>Negative    2375<br>Name: label, dtype: int64<br><br>Sentence length at quantile 0.91: 183.<br></code></pre></td></tr></table></figure><h3 id="Using-LSTM-Model"><a href="#Using-LSTM-Model" class="headerlink" title="Using LSTM Model"></a>Using LSTM Model</h3><p>Next, we employ the LSTM (Long Short-Term Memory) model from deep learning for sentiment analysis on the provided dataset. The complete Python code for this process is provided:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> LSTM, Dense, Embedding, Dropout<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># 导入数据</span><br><span class="hljs-comment"># 文件的数据中，特征为evaluation, 类别为label.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filepath, input_shape=<span class="hljs-number">20</span></span>):<br>    df = pd.read_csv(filepath)<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;label&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 构造字符级别的特征</span><br>    string = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocabulary:<br>        string += word<br><br>    vocabulary = <span class="hljs-built_in">set</span>(string)<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(word_dictionary, f)<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(label_dictionary, f)<br>    output_dictionary = &#123;i: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys()) <span class="hljs-comment"># 词汇表大小</span><br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys()) <span class="hljs-comment"># 标签类别数量</span><br><br>    <span class="hljs-comment"># 序列填充，按input_shape填充，长度不足的按0补充</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;evaluation&#x27;</span>]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[sent]] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;label&#x27;</span>]]<br>    y = [np_utils.to_categorical(label, num_classes=label_size) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br>    y = np.array([<span class="hljs-built_in">list</span>(_[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y])<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><span class="hljs-comment"># 创建深度学习模型， Embedding + LSTM + Softmax.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_LSTM</span>(<span class="hljs-params">n_units, input_shape, output_dim, filepath</span>):<br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(LSTM(n_units, input_shape=(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>])))<br>    model.add(Dropout(<span class="hljs-number">0.2</span>))<br>    model.add(Dense(label_size, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    plot_model(model, to_file=<span class="hljs-string">&#x27;./model_lstm.png&#x27;</span>, show_shapes=<span class="hljs-literal">True</span>)<br>    model.summary()<br><br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>(<span class="hljs-params">input_shape, filepath, model_save_path</span>):<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    <span class="hljs-comment"># input_shape = 100</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)<br>    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = <span class="hljs-number">0.1</span>, random_state = <span class="hljs-number">42</span>)<br><br>    <span class="hljs-comment"># 模型输入参数，需要自己根据需要调整</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">5</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    lstm_model.save(model_save_path)<br><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    predict = []<br>    label = []<br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        label_predict = output_dictionary[np.argmax(y_predict[<span class="hljs-number">0</span>])]<br>        label_true = output_dictionary[np.argmax(test_y[start:end])]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(sentence), label_true, label_predict) <span class="hljs-comment"># 输出预测结果</span><br>        predict.append(label_predict)<br>        label.append(label_true)<br><br>    acc = accuracy_score(predict, label) <span class="hljs-comment"># 预测准确率</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;模型在测试集上的准确率为: %s.&#x27;</span> % acc)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    filepath = <span class="hljs-string">&#x27;./corpus.csv&#x27;</span><br>    input_shape = <span class="hljs-number">180</span><br>    model_save_path = <span class="hljs-string">&#x27;./corpus_model.h5&#x27;</span><br>    model_train(input_shape, filepath, model_save_path)<br></code></pre></td></tr></table></figure><p>For the aforementioned model, it was trained 5 times with a training-to-testing set ratio of 9:1, yielding the following output:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs YAML"><span class="hljs-string">......</span><br><span class="hljs-string">Epoch</span> <span class="hljs-number">5</span><span class="hljs-string">/5</span><br><span class="hljs-string">......</span><br><span class="hljs-number">3424</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 2s - loss: 0.1280 - acc:</span> <span class="hljs-number">0.9565</span><br><span class="hljs-number">3456</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3488</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9570</span><br><span class="hljs-number">3520</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1287 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3552</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1290 - acc:</span> <span class="hljs-number">0.9564</span><br><span class="hljs-number">3584</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3616</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3648</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1278 - acc:</span> <span class="hljs-number">0.9572</span><br><span class="hljs-number">3680</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1271 - acc:</span> <span class="hljs-number">0.9576</span><br><span class="hljs-number">3712</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1268 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3744</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9575</span><br><span class="hljs-number">3776</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1272 - acc:</span> <span class="hljs-number">0.9579</span><br><span class="hljs-number">3808</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3840</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1281 - acc:</span> <span class="hljs-number">0.9581</span><br><span class="hljs-number">3854</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">18s 5ms/step - loss: 0.1298 - acc:</span> <span class="hljs-number">0.9577</span><br><span class="hljs-string">......</span><br><span class="hljs-string">给父母买的，特意用了一段时间再来评价，电视非常好，没有坏点和损坏，界面也很简洁，便于操作，稍微不足就是开机会比普通电视慢一些，这应该是智能电视的通病吧，如果可以希望微鲸大大可以更新系统优化下开机时间~电视真的很棒，性价比爆棚，值得大家考虑购买。</span> <span class="hljs-string">客服很细心，快递小哥很耐心的等我通电验货，态度非常好。</span> <span class="hljs-string">负面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">长须鲸和海狮回答都很及时，虽然物流不够快但是服务不错电视不错，对比了乐视小米和微鲸论性价比还是微鲸好点</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">所以看不到4k效果，但是应该可以。</span> <span class="hljs-string">自带音响，中规中矩吧，好像没有别人说的好。而且，到现在没连接上我的漫步者，这个非常不满意，因为看到网上说好像普通3.5mm的连不上或者连上了声音小。希望厂家接下来开发的电视有改进。不知道我要不要换个音响。其他的用用再说。</span> <span class="hljs-string">放在地上的是跟我混了两年的tcl，天气受潮，修了一次，下岗了。</span> <span class="hljs-string">最后，我也觉得底座不算太稳，凑合着用。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">电视机一般，低端机不要求那么高咯。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">很好，两点下单上午就到了，服务很好。</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">帮朋友买的，好好好好好好好好</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">......</span><br><span class="hljs-string">模型在测试集上的准确率为:</span> <span class="hljs-number">0.9020979020979021</span><span class="hljs-string">.</span><br></code></pre></td></tr></table></figure><p>The model achieved an accuracy of over 95% on the training set and over 90% on the testing set, indicating a fairly good performance.</p><h3 id="Model-Prediction"><a href="#Model-Prediction" class="headerlink" title="Model Prediction"></a>Model Prediction</h3><p>Subsequently, we use the trained model to predict the sentiment polarity of new data. The Python code for sentiment prediction is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">180</span><br>    sent = <span class="hljs-string">&quot;电视刚安装好，说实话，画质不怎么样，很差！&quot;</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = <span class="hljs-string">&#x27;./sentiment_analysis.h5&#x27;</span><br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br>    label_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> output_dictionary.items()&#125;<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入语句: %s&#x27;</span> % sent)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;情感预测结果: %s&#x27;</span> % label_dict[np.argmax(y_predict)])<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有汉字不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>The output result would be:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs markdown">输入语句: 电视刚安装好，说实话，画质不怎么样，很差！<br>情感预测结果: 负面<br></code></pre></td></tr></table></figure><p>Let’s try testing a few other comments:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs markdown">输入语句: 物超所值，真心不错<br>情感预测结果: 正面<br>输入语句: 很大很好，方便安装！<br>情感预测结果: 正面<br>输入语句: 卡，慢，死机，闪退。<br>情感预测结果: 负面<br>输入语句: 这种货色就这样吧，别期待怎样。<br>情感预测结果: 负面<br>输入语句: 啥服务态度码，出了事情一个推一个，送货安装还收我50<br>情感预测结果: 负面<br>输入语句: 京东服务很好！但我买的这款电视两天后就出现这样的问题，很后悔买了这样的电视<br>情感预测结果: 负面<br>输入语句: 产品质量不错，就是这位客服的态度十分恶劣，对相关服务不予解释说明，缺乏耐心，<br>情感预测结果: 负面<br>输入语句: 很满意，电视非常好。护眼模式，很好，也很清晰。<br>情感预测结果: 负面<br></code></pre></td></tr></table></figure><p>Thank you for reading!</p>]]></content>
    
    
    <categories>
      
      <category>Natural language processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP: Bag of Words Model</title>
    <link href="/2023/10/12/NLP-Bag-of-Words-Model/"/>
    <url>/2023/10/12/NLP-Bag-of-Words-Model/</url>
    
    <content type="html"><![CDATA[<p>This article will introduce the common Bag of Words (BoW) model in NLP and how to utilize it for calculating the similarity between sentences using cosine similarity.</p><p>Firstly, let’s delve into what the Bag of Words model is. Let’s consider two simple sentences for example:</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent1</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br><span class="hljs-attribute">sent2</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br></code></pre></td></tr></table></figure><p>In NLP, handling complete paragraphs or sentences at once is often challenging. Hence, the first step typically involves tokenization and word segmentation. Here, as we only have sentences, we’ll focus on tokenization. For English sentences, the <code>word_tokenize</code> function from NLTK can be used, while for Chinese sentences, the <code>jieba</code> module can be utilized. Therefore, the initial step is tokenization, with code as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br></code></pre></td></tr></table></figure><p>The output result would be:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>], [<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]]<br></code></pre></td></tr></table></figure><p>Tokenization is complete. The next step involves constructing a corpus, which consists of all words and punctuation appearing in the sentences. The code for creating the corpus is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">all_list = []<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    all_list += text<br><br>corpus = <span class="hljs-built_in">set</span>(all_list)<br><span class="hljs-built_in">print</span>(corpus)<br></code></pre></td></tr></table></figure><p>The output would be a set containing all unique words and punctuation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>The following step is to establish a numerical mapping for the words and punctuation in the corpus. This aids in the subsequent vector representation of sentences. The code for creating the mapping is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(corpus, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(corpus))))<br><span class="hljs-built_in">print</span>(corpus_dict)<br></code></pre></td></tr></table></figure><p>The output would be a dictionary mapping each unique word or punctuation to a numerical value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;running&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;reading&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;love&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;sky&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;.&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;I&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;like&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;sea&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;,&#x27;</span>: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure><p>Although the words and punctuation aren’t mapped based on their order of appearance, it won’t affect the vector representation of sentences and subsequent sentence similarity.</p><p>The next crucial step in the Bag of Words model is to establish vector representations for sentences. This representation doesn’t merely select 0s and 1s based on the presence of words or punctuation; instead, it considers the frequency of their appearance as their numerical representation. Combining the corpus dictionary previously created, the code for vector representation of sentences is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Establishing vector representation for sentences</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_rep</span>(<span class="hljs-params">text, corpus_dict</span>):<br>    vec = []<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> corpus_dict.keys():<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> text:<br>            vec.append((corpus_dict[key], text.count(key)))<br>        <span class="hljs-keyword">else</span>:<br>            vec.append((corpus_dict[key], <span class="hljs-number">0</span>))<br><br>    vec = <span class="hljs-built_in">sorted</span>(vec, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">return</span> [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> vec]<br><br>vec1 = vector_rep(texts[<span class="hljs-number">0</span>], corpus_dict)<br>vec2 = vector_rep(texts[<span class="hljs-number">1</span>], corpus_dict)<br><span class="hljs-built_in">print</span>(vec1)<br><span class="hljs-built_in">print</span>(vec2)<br></code></pre></td></tr></table></figure><p>The output would represent the true vector representations of the sentences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><p>Let’s pause for a moment and observe this vector. In the first sentence, “I” appears twice. In the corpus dictionary, “I” corresponds to the number 5, thus “5” appears twice in the first sentence, resulting in the tuple <code>(5, 2)</code> in the list, indicating that the word “I” appears twice in the first sentence. The true vector representations of the two sentences are:<br>[2, 0, 0, 1, 1, 2, 0, 1, 1]<br>[1, 1, 1, 0, 1, 2, 1, 0, 1]</p><p>Now, the Bag of Words model is complete. Next, we’ll utilize this model, specifically the vector representations of the two sentences, to calculate their similarity.</p><p>In NLP, when two sentences are represented as vectors, cosine similarity is often chosen as a measure of similarity. The cosine similarity of vectors is the cosine value of the angle between the vectors. The Python code for calculating cosine similarity is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">similarity_with_2_sents</span>(<span class="hljs-params">vec1, vec2</span>):<br>    inner_product = <span class="hljs-number">0</span><br>    square_length_vec1 = <span class="hljs-number">0</span><br>    square_length_vec2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> tup1, tup2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(vec1, vec2):<br>        inner_product += tup1 * tup2<br>        square_length_vec1 += tup1**<span class="hljs-number">2</span><br>        square_length_vec2 += tup2**<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> (inner_product / (sqrt(square_length_vec1 * square_length_vec2)))<br><br>cosine_sim = similarity_with_2_sents(vec1, vec2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The cosine similarity between the two sentences is: %.4f.&#x27;</span> % cosine_sim)<br></code></pre></td></tr></table></figure><p>The output would be:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">The cosine similarity between the two sentences <span class="hljs-keyword">is</span>: <span class="hljs-number">0.7303</span>.<br></code></pre></td></tr></table></figure><p>Thus, we’ve obtained the similarity between sentences using the Bag of Words model.</p><p>However, in practical NLP projects, for computing sentence similarity, one can easily use the <code>gensim</code> module. Below is the code using <code>gensim</code> to calculate similarity between two sentences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br><br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br><span class="hljs-comment"># Building the corpus</span><br>dictionary = corpora.Dictionary(texts)<br><br><span class="hljs-comment"># Using doc2bow as the Bag of Words model</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>similarity = Similarity(<span class="hljs-string">&#x27;-Similarity-index&#x27;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><br><span class="hljs-comment"># Obtaining the similarity between sentences</span><br>new_sentence = sent1<br>test_corpus_1 = dictionary.doc2bow(word_tokenize(new_sentence))<br><br>cosine_sim = similarity[test_corpus_1][<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Similarity between the two sentences using gensim: %.4f.&quot;</span> % cosine_sim)<br></code></pre></td></tr></table></figure><p>The output would be:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Similarity between the two sentences using gensim: <span class="hljs-number">0.7303</span>.<br></code></pre></td></tr></table></figure><p>Thank you for reading!</p>]]></content>
    
    
    <categories>
      
      <category>Natural language processing</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DW: Common Commands of HiveQL</title>
    <link href="/2023/09/28/DW-Common-Commands-of-HiveQL/"/>
    <url>/2023/09/28/DW-Common-Commands-of-HiveQL/</url>
    
    <content type="html"><![CDATA[<p>HiveQL, the SQL-like language for querying data in Apache Hive, empowers users to interact with structured data in a distributed computing environment. Below are some frequently used commands:</p><h2 id="Data-Definition-Language-DDL-Commands"><a href="#Data-Definition-Language-DDL-Commands" class="headerlink" title="Data Definition Language (DDL) Commands"></a>Data Definition Language (DDL) Commands</h2><h3 id="1-CREATE-TABLE"><a href="#1-CREATE-TABLE" class="headerlink" title="1. CREATE TABLE"></a>1. CREATE TABLE</h3><p>Creates a new table in Hive.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> my_table (<br>  id <span class="hljs-type">INT</span>,<br>  name STRING,<br>  age <span class="hljs-type">INT</span><br>);<br></code></pre></td></tr></table></figure><h3 id="2-ALTER-TABLE"><a href="#2-ALTER-TABLE" class="headerlink" title="2. ALTER TABLE"></a>2. ALTER TABLE</h3><p>Modifies an existing table by adding, renaming, or dropping columns.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">ALTER</span> <span class="hljs-keyword">TABLE</span> my_table<br><span class="hljs-keyword">ADD</span> <span class="hljs-keyword">COLUMN</span> email STRING;<br></code></pre></td></tr></table></figure><h3 id="3-DROP-TABLE"><a href="#3-DROP-TABLE" class="headerlink" title="3. DROP TABLE"></a>3. DROP TABLE</h3><p>Deletes a table from the database.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DROP</span> <span class="hljs-keyword">TABLE</span> my_table;<br></code></pre></td></tr></table></figure><h2 id="Data-Manipulation-Language-DML-Commands"><a href="#Data-Manipulation-Language-DML-Commands" class="headerlink" title="Data Manipulation Language (DML) Commands"></a>Data Manipulation Language (DML) Commands</h2><h3 id="1-INSERT-INTO"><a href="#1-INSERT-INTO" class="headerlink" title="1. INSERT INTO"></a>1. INSERT INTO</h3><p>Adds new rows of data into a table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">INSERT</span> <span class="hljs-keyword">INTO</span> my_table (id, name, age, email)<br><span class="hljs-keyword">VALUES</span> (<span class="hljs-number">1</span>, <span class="hljs-string">&#x27;John Doe&#x27;</span>, <span class="hljs-number">30</span>, <span class="hljs-string">&#x27;john@example.com&#x27;</span>);<br></code></pre></td></tr></table></figure><h3 id="2-SELECT"><a href="#2-SELECT" class="headerlink" title="2. SELECT"></a>2. SELECT</h3><p>Retrieves data from one or more tables based on specified conditions.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> <span class="hljs-operator">*</span> <span class="hljs-keyword">FROM</span> my_table <span class="hljs-keyword">WHERE</span> age <span class="hljs-operator">&gt;</span> <span class="hljs-number">25</span>;<br></code></pre></td></tr></table></figure><h3 id="3-UPDATE"><a href="#3-UPDATE" class="headerlink" title="3. UPDATE"></a>3. UPDATE</h3><p>Modifies existing data in a table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">UPDATE</span> my_table <span class="hljs-keyword">SET</span> age <span class="hljs-operator">=</span> <span class="hljs-number">31</span> <span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><h3 id="4-DELETE"><a href="#4-DELETE" class="headerlink" title="4. DELETE"></a>4. DELETE</h3><p>Removes rows from a table based on specified conditions.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">DELETE</span> <span class="hljs-keyword">FROM</span> my_table <span class="hljs-keyword">WHERE</span> id <span class="hljs-operator">=</span> <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure><h3 id="5-JOIN"><a href="#5-JOIN" class="headerlink" title="5. JOIN"></a>5. JOIN</h3><p>Combines rows from two or more tables based on a related column between them.</p><h4 id="Inner-Join"><a href="#Inner-Join" class="headerlink" title="Inner Join"></a>Inner Join</h4><p>Retrieves rows that have matching values in both tables.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> table_a a<br><span class="hljs-keyword">INNER</span> <span class="hljs-keyword">JOIN</span> table_b b <span class="hljs-keyword">ON</span> a.id <span class="hljs-operator">=</span> b.id;<br></code></pre></td></tr></table></figure><h4 id="Left-Join-or-Left-Outer-Join"><a href="#Left-Join-or-Left-Outer-Join" class="headerlink" title="Left Join (or Left Outer Join)"></a>Left Join (or Left Outer Join)</h4><p>Retrieves all rows from the left table and matching rows from the right table. If there’s no match, NULL values are returned for the right table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> table_a a<br><span class="hljs-keyword">LEFT</span> <span class="hljs-keyword">JOIN</span> table_b b <span class="hljs-keyword">ON</span> a.id <span class="hljs-operator">=</span> b.id;<br></code></pre></td></tr></table></figure><h4 id="Right-Join-or-Right-Outer-Join"><a href="#Right-Join-or-Right-Outer-Join" class="headerlink" title="Right Join (or Right Outer Join)"></a>Right Join (or Right Outer Join)</h4><p>Retrieves all rows from the right table and matching rows from the left table. If there’s no match, NULL values are returned for the left table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> table_a a<br><span class="hljs-keyword">RIGHT</span> <span class="hljs-keyword">JOIN</span> table_b b <span class="hljs-keyword">ON</span> a.id <span class="hljs-operator">=</span> b.id;<br></code></pre></td></tr></table></figure><h4 id="Full-Join-or-Full-Outer-Join"><a href="#Full-Join-or-Full-Outer-Join" class="headerlink" title="Full Join (or Full Outer Join)"></a>Full Join (or Full Outer Join)</h4><p>Retrieves all rows when there is a match in either the left or right table. If there’s no match, NULL values are returned for the unmatched side.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> a.<span class="hljs-operator">*</span>, b.<span class="hljs-operator">*</span><br><span class="hljs-keyword">FROM</span> table_a a<br><span class="hljs-keyword">FULL</span> <span class="hljs-keyword">OUTER</span> <span class="hljs-keyword">JOIN</span> table_b b <span class="hljs-keyword">ON</span> a.id <span class="hljs-operator">=</span> b.id;<br></code></pre></td></tr></table></figure><h3 id="6-UNION"><a href="#6-UNION" class="headerlink" title="6. UNION"></a>6. UNION</h3><p>Combines the results of two or more SELECT statements into a single result set.</p><h2 id="Additional-Commands"><a href="#Additional-Commands" class="headerlink" title="Additional Commands"></a>Additional Commands</h2><h3 id="1-SHOW-DATABASES"><a href="#1-SHOW-DATABASES" class="headerlink" title="1. SHOW DATABASES"></a>1. SHOW DATABASES</h3><p>Lists all databases in the Hive environment.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SHOW</span> DATABASES;<br></code></pre></td></tr></table></figure><h3 id="2-SHOW-PARTITIONS"><a href="#2-SHOW-PARTITIONS" class="headerlink" title="2. SHOW PARTITIONS"></a>2. SHOW PARTITIONS</h3><p>Displays partition information for a specified table.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SHOW</span> PARTITIONS table_name;<br></code></pre></td></tr></table></figure><h3 id="3-SET"><a href="#3-SET" class="headerlink" title="3. SET"></a>3. SET</h3><p>Configures Hive parameters by setting values.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SET</span> parameter_name <span class="hljs-operator">=</span> <span class="hljs-keyword">value</span>;<br></code></pre></td></tr></table></figure><h3 id="4-QUIT"><a href="#4-QUIT" class="headerlink" title="4. QUIT"></a>4. QUIT</h3><p>Exits the Hive environment.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">QUIT;<br></code></pre></td></tr></table></figure><p>These detailed HiveQL commands provide a comprehensive understanding of data manipulation techniques, particularly JOIN operations like Inner Join, Left Join, Right Join, and Full Join. Utilizing these operations in Apache Hive enables users to handle complex data relationships and merge data sets efficiently in distributed environments.</p>]]></content>
    
    
    <categories>
      
      <category>Data Warehouse</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DW</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DW: Introduction to Apache Hive</title>
    <link href="/2023/09/15/DW-Introduction-to-Apache-Hive/"/>
    <url>/2023/09/15/DW-Introduction-to-Apache-Hive/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction-to-Hive"><a href="#1-Introduction-to-Hive" class="headerlink" title="1. Introduction to Hive"></a>1. Introduction to Hive</h2><p>Apache Hive is an open-source data warehousing system built on top of Hadoop. It translates structured and semi-structured data files stored in Hadoop files into a database table structure. It provides a SQL-like querying model known as Hive Query Language (HQL), allowing access and analysis of large datasets stored in Hadoop files.</p><p>The core of Hive involves transforming HQL into MapReduce programs, which are then submitted to the Hadoop cluster for execution. Facebook initially implemented Hive and later open-sourced it.</p><p>Why use Hive?</p><ul><li>Challenges with direct processing of data using Hadoop MapReduce<ul><li>High learning curve requiring proficiency in Java</li><li>Complexity in implementing complex query logic using MapReduce</li></ul></li><li>Benefits of using Hive for data processing<ul><li>Offers a SQL-like interface, enabling rapid development (simple and user-friendly)</li><li>Reduces the learning curve by avoiding direct MapReduce coding</li><li>Supports custom functions for easy functionality extension</li><li>Leverages Hadoop’s strength in storing and analyzing massive datasets</li></ul></li></ul><h2 id="2-Hive-Architecture-and-Components"><a href="#2-Hive-Architecture-and-Components" class="headerlink" title="2. Hive Architecture and Components"></a>2. Hive Architecture and Components</h2><ul><li><p>User Interfaces<br>Include CLI, JDBC&#x2F;ODBC, WebGUI. CLI (command-line interface) operates as a shell command line. The Thrift server in Hive allows external clients to interact with Hive via a network, similar to JDBC or ODBC protocols. WebGUI enables Hive access through a browser.</p></li><li><p>Metadata Storage<br>Usually stored in relational databases such as MySQL&#x2F;Derby. Hive’s metadata comprises table names, columns, partitions and their attributes, table properties (e.g., external table), and the directory where the table data resides.</p></li><li><p>Driver<br>Comprises syntax parser, plan compiler, optimizer, and executor. It handles HQL query execution from lexical and syntax analysis to compilation, optimization, and query plan generation. The generated query plan is stored in HDFS and executed by the execution engine subsequently.</p></li><li><p>Execution Engine<br>Hive doesn’t directly process data files but employs an execution engine. Currently, Hive supports three execution engines: MapReduce, Tez, and Spark.</p></li></ul><h2 id="3-Data-Model"><a href="#3-Data-Model" class="headerlink" title="3. Data Model"></a>3. Data Model</h2><p>The data model serves as the framework to describe, organize, and manipulate data, presenting a description of the characteristics of real-world data.</p><p>In Hive, the data model exhibits resemblances to RDBMS table structures, yet it also encompasses its distinct model. The data within Hive is categorized into three primary granular levels:</p><ul><li><strong>Tables:</strong> Basic organizational units storing data and defining its schema.</li><li><strong>Partitions:</strong> Logically segregate data within tables, optimizing query performance.</li><li><strong>Buckets:</strong> Divide tables into manageable parts based on hash functions, aiding efficient querying.</li></ul><p>The data model offers flexibility in data organization and access.</p>]]></content>
    
    
    <categories>
      
      <category>Data Warehouse</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DW</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DW: Introduction to Data Warehouses</title>
    <link href="/2023/09/12/DW-Introduction-to-Data-Warehouses/"/>
    <url>/2023/09/12/DW-Introduction-to-Data-Warehouses/</url>
    
    <content type="html"><![CDATA[<p>In this article, we’re diving into the world of data warehousing. We’ll break down the basics, explore how it all started, and uncover why it’s become such a big deal in the business world. </p><p>A data warehouse, often abbreviated as DW, is a system designed for storing, analyzing, and reporting data effectively.</p><p>The main goal of a data warehouse is to create an integrated data environment made specifically for analysis, aiding crucial decision-making in businesses.</p><h2 id="1-Understanding-How-DW-Began"><a href="#1-Understanding-How-DW-Began" class="headerlink" title="1. Understanding How DW Began"></a>1. Understanding How DW Began</h2><p>A data warehouse doesn’t create data itself; it gathers data from various external systems. Similarly, it doesn’t consume data directly; instead, its outcomes are shared with different applications. This is why it’s called a “warehouse” and not a “factory.”</p><p>In simple terms: its purpose is analyzing data to support business decisions.</p><p>Within a business setting, information usually serves two main purposes: maintaining records and guiding analytical decisions.</p><p>For instance, let’s look at China Life Insurance Company, where I previously interned, to understand why data warehousing emerged. They manage several lines of business, like life insurance, property insurance, auto insurance, and pension insurance. Smooth operations across these areas need careful handling of vast amounts of customer information, policy records, financial transactions, and claims data.</p><p><img src="/images/DW1.jpg" alt="Structure of DW"></p><p>Online transaction systems (OLTP) handle these operations well by processing transactions swiftly. These systems process user data quickly, providing prompt results.</p><p>Common databases like Oracle, MySQL, and SQL Server are typical OLTP tools.</p><p>As the group’s business grows, so does the volume of data. This growth poses challenges:</p><ul><li>Identifying issues in insurance categories.</li><li>Crafting effective policies.</li><li>Detecting potential fraud.</li><li>Ensuring reports cover all business lines.</li></ul><p>OLTP systems focus on transactions. All business operations involve reading and writing data. Reading operations usually exert more pressure. However, conducting various analyses directly within an OLTP environment raises concerns:</p><ul><li>Analyzing data means more reading, increasing pressure.</li><li>OLTP systems store data briefly.</li><li>Data is scattered across systems with inconsistent structures.</li></ul><p>While analyzing smaller datasets within an OLTP environment might work, building an integrated data analytics platform becomes crucial for larger-scale analysis without affecting OLTP systems.</p><p>This platform aims to focus on analysis, supporting analytical tasks without impacting OLTP systems. This marked the beginning of data warehousing in enterprises.</p><h2 id="2-Creating-a-DW"><a href="#2-Creating-a-DW" class="headerlink" title="2. Creating a DW"></a>2. Creating a DW</h2><p>A data warehouse specializes in storing, analyzing, and reporting data, creating an integrated environment for analysis—part of a broader category called OLAP (Online Analytical Processing) systems.</p><p>For instance, China Life Insurance Company can create a data warehouse tailored for analytical decision-making.</p><h2 id="3-Structure-of-the-DW"><a href="#3-Structure-of-the-DW" class="headerlink" title="3. Structure of the DW"></a>3. Structure of the DW</h2><p><strong>ODS Layer (Operation Data Store):</strong> This layer stores raw, unprocessed data in the data warehouse system, aligning with the source system’s structure. It prepares data for the warehouse while recording historical changes.</p><p><strong>DW Layer (Data Warehouse):</strong> Derived from the ODS layer, it manages data processing and integration. It establishes consistent dimensions, creates detailed fact tables for analysis, and consolidates common indicators. </p><p><strong>ADS Layer:</strong> This layer provides tailored business data for end-users, including reports, charts, KPIs, and data analysis tools.</p><p>Benefits of Layering:</p><ul><li>Organized data structure.</li><li>Tracing data sources.</li><li>Reducing repetitive work.</li><li>Simplifying tasks for better maintenance.</li><li>Shielding original data from issues.</li></ul><p>Thank you for reading!</p>]]></content>
    
    
    <categories>
      
      <category>Data Warehouse</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DW</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Step-by-Step: Using Hexo and GitHub to Create Your Website</title>
    <link href="/2023/09/10/Step-by-Step-Using-Hexo-and-GitHub-to-Create-Your-Website/"/>
    <url>/2023/09/10/Step-by-Step-Using-Hexo-and-GitHub-to-Create-Your-Website/</url>
    
    <content type="html"><![CDATA[<p>Hi everyone! I’m excited to share that I’ve created my own website today! Welcome to my blog!</p><p>In this post, I’ll share my step-by-step approach to using Hexo and GitHub to build it. </p><h2 id="1-Preparation"><a href="#1-Preparation" class="headerlink" title="1. Preparation"></a>1. Preparation</h2><ol><li><p><strong>GitHub Account</strong></p><p>You’ll need a GitHub account. If you don’t have one, head over to the official website <a href="http://github.com/">http://github.com</a> to register.</p></li><li><p><strong>Install Git</strong></p><p>Install Git on your computer. I used homebrew for installation.</p></li><li><p><strong>Install Node.js</strong></p><p>Install Node.js on your computer. You can also use homebrew for this installation.</p></li></ol><h2 id="2-Create-Repository"><a href="#2-Create-Repository" class="headerlink" title="2. Create Repository"></a>2. Create Repository</h2><p>To set up your website:</p><ol><li><p><strong>Create Repository:</strong> Go to your GitHub account and navigate to “Your repositories” to access the repositories page. Create a new repository named <code>&lt;username&gt;.github.io</code>.</p></li><li><p><strong>Create Homepage File:</strong> Within this repository, create a new file named <code>index.html</code>. This file will serve as the homepage for your website.</p></li><li><p><strong>Add Content to Homepage:</strong> Populate the <code>index.html</code> file with some basic content. For instance:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span>Hello<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span>I&#x27;m xxx, and this is my personal website.<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure></li><li><p><strong>Commit Changes:</strong> After adding content, click “Commit new file” to save these changes to the repository.</p></li><li><p><strong>Access Your Homepage:</strong> Your newly created homepage’s address can be found in GitHub Pages at https:&#x2F;&#x2F;[your_username].github.io&#x2F;</p></li></ol><h2 id="3-Configure-SSH"><a href="#3-Configure-SSH" class="headerlink" title="3. Configure SSH"></a>3. Configure SSH</h2><p>To enable remote file uploads from your computer to your GitHub repository, configure SSH by following these steps:</p><ol><li>Click on your personal GitHub account’s “Settings” option.</li><li>Navigate to “SSH and GPG keys” to configure your SSH public key.</li></ol><h2 id="4-Blog-Initialization"><a href="#4-Blog-Initialization" class="headerlink" title="4. Blog Initialization"></a>4. Blog Initialization</h2><p>We’ll use Hexo to create your blog website. Hexo is a static blog site generator based on Node.js. You can find its official website at <a href="https://hexo.io/">https://hexo.io</a>.</p><p>Create an empty folder, for instance, <code>github_blog</code>. Use the <code>hexo init</code> command to initialize your blog. The contents of the initialized folder are described as follows:</p><ul><li><code>_config.yml</code>: Configuration information for the website, where you can set most parameters.</li><li><code>package.json</code>: Application information.</li><li><code>scaffolds</code>: Folder for templates. Hexo uses these to create files when you create a new post.</li><li><code>source</code>: Folder for resources.</li><li><code>themes</code>: Folder for themes, which Hexo uses to generate static pages.</li></ul><h2 id="5-Generating-Your-Personal-Blog-Website"><a href="#5-Generating-Your-Personal-Blog-Website" class="headerlink" title="5. Generating Your Personal Blog Website"></a>5. Generating Your Personal Blog Website</h2><p>Edit the <code>_config.yml</code> file with the following deployment information:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/[your_username]/[your_repository_name]</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">main</span><br></code></pre></td></tr></table></figure><p>After installing the plugin <code>npm install hexo-deployer-git --save</code>, run the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean    <span class="hljs-comment"># Clean up data</span><br>hexo d -g    <span class="hljs-comment"># Generate the blog</span><br></code></pre></td></tr></table></figure><p>At this point, your blog data will be pushed to GitHub. The empty repository created in the first step will now have content. You can access your website at <a href="https://username.github.io/">https://username.github.io/</a>, where ‘username’ is your GitHub username.</p><h2 id="6-Create-a-New-Post"><a href="#6-Create-a-New-Post" class="headerlink" title="6. Create a New Post"></a>6. Create a New Post</h2><p>Let’s create a blog post titled “Hello world!” using the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&quot;Hello world!&quot;</span><br></code></pre></td></tr></table></figure><p>This will generate a <code>Hello world!.md</code> file in the <code>source/_posts</code> folder within your current directory. The file will have the following content:</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs markdown">---<br>title: Hello world!<br>date: 2023-09-02 18:32:29<br><span class="hljs-section">tags: [empty]</span><br><span class="hljs-section">---</span><br></code></pre></td></tr></table></figure><p>After the <code>---</code>, you can write the content of your blog post using Markdown format.</p><p>Once you’ve finished writing your blog post, use the following commands:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean    <span class="hljs-comment"># Clean up data</span><br>hexo d -g    <span class="hljs-comment"># Generate the blog</span><br></code></pre></td></tr></table></figure><p>This will update your personal blog with the newly added post.</p><p>I hope this guide helps you. Happy website building!</p>]]></content>
    
    
    <categories>
      
      <category>Website</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Website</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
